\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage{booktabs}
\usepackage{array}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Variational JEPA: Adding Uncertainty to Visual Representations}

\author{
\IEEEauthorblockN{Vladislav Kalinichenko, Polina Korobeinikova, Alexandra Starikova-Nasibullina, Timur Struchkov}
\IEEEauthorblockA{\textit{Innopolis University}\\
Innopolis, Russia\\
\{v.kalinichenko, p.korobeinikova, a.nasibullina, t.struchkov\}@innopolis.university}
}

\maketitle

\begin{abstract}
Self-supervised Joint-Embedding Predictive Architectures (JEPAs) learn semantic representations without pixel-level reconstruction, yet the standard I-JEPA predictor is deterministic and discards multimodal uncertainty.
We implement four probabilistic predictors on top of a ViT-based I-JEPA backbone to model latent distributions for masked image blocks: (i) a multivariate Mixture Density Network (MDN), (ii) an autoregressive masked MLP (RNADE-style), (iii) a RealNVP-style normalizing flow trained with flow matching, and (iv) a diffusion head that denoises latent embeddings.
All models share Tiny ImageNet-100 data, masking, and EMA teacher--student infrastructure.
All implementations have shown the ability to model multimodal, diverse distributions for latent vectors, without collapsing, and achieve a comparable performance with the baseline.

\end{abstract}

\section{Introduction}
Joint-Embedding Predictive Architectures (JEPAs)~\cite{assran2023jepa} learn by predicting teacher embeddings for masked target blocks while observing only a partially visible context.
Unlike pixel-space autoencoders or diffusion models, JEPAs avoid reconstructing low-level detail and focus the predictor on semantic alignment in latent space.
The core pipeline (Figure~\ref{fig:jepa_overview}) keeps a momentum-updated teacher that encodes the full image, a student that sees only context patches, and a predictor that maps masked target queries to the teacher's latent targets; decorrelation terms prevent trivial collapse and no negative pairs are required, simplifying optimization compared to contrastive SSL.

\begin{figure}[t]
    \centering
    \fbox{\begin{minipage}[c][1.6in][c]{0.45\textwidth}\centering
    Placeholder for JEPA overview: masked context to student, full image to EMA teacher, predictor maps context+queries to teacher targets in latent space.
    \end{minipage}}
    \caption{High-level JEPA schematic (replace with final figure): the student sees only context patches, the EMA teacher encodes the full image, and the predictor aligns student outputs to teacher targets for masked blocks.}
    \label{fig:jepa_overview}
\end{figure}

This latent-space formulation has two advantages: (i) it reduces background leakage and pixel-level shortcuts that plague reconstruction-based pretraining, and (ii) it keeps inductive biases about semantics aligned with downstream tasks such as classification or detection.
However, the standard I-JEPA predictor is deterministic and emits a single vector per masked block, even when the scene supports multiple plausible completions (e.g., occluded limbs, textured backgrounds, or ambiguous object categories).
Such a single-mode solution discards epistemic uncertainty and pushes the model toward overconfident guesses.
Sampling from a learned distribution instead enables calibrated masks that better cover the teacher manifold: diverse draws can be used for Monte Carlo risk estimates, active data selection, or downstream generative editing where multiple hypotheses are genuinely useful.

Guided by these goals, we augment I-JEPA with probabilistic predictors that output sampleable distributions over latent embeddings while retaining the same ViT backbone and masking policy.
We deliberately cover four complementary density-modeling families, covering the most popular and fundamental distribution modeling approaches in deep learning: (i) MDNs as simple mixture models, (ii) autoregressive masked MLPs as factorized density estimators, (iii) invertible flows with exact likelihoods, and (iv) latent diffusion as an iterative denoiser that tends to capture complex multi-modal structure.
We follow the BYOL/MoCo-style teacher--student recipe~\cite{grill2020byol,chen2020moco} and swap only the predictor head.
This report summarizes the design and results of four predictors trained on Tiny ImageNet-100 and compares how each captures multimodal uncertainty.

\section{Related Work}
\textbf{Joint-embedding SSL.}
I-JEPA~\cite{assran2023jepa} anchors the teacher--student/EMA design from BYOL and MoCo~\cite{grill2020byol,chen2020moco,he2020momentum} but moves prediction to latent space: masking removes target tokens from the student, positional encodings are added to learned queries, and the predictor regresses to teacher targets.
Because it forgoes contrastive negatives and pixel reconstruction, I-JEPA produces representations with strong invariances while remaining simpler to optimize than pixel-aware masked autoencoders (MAE)~\cite{he2022mae}.
Our work keeps this architecture intact and experiments with alternative predictors that can be sampled, effectively turning JEPAs into conditional density estimators over teacher latents.
Throughout this section we distinguish predictors that yield an explicit parametric distribution in a single forward pass (MDNs and flows) from those that define a distribution implicitly through a stochastic sampling procedure (autoregressive and diffusion heads).

\textbf{Autoregressive density estimators.}
Real-valued neural autoregressive density-estimator (MADE/RNADE)~\cite{germain2015made,uria2014rnade} factorize a joint density into a product of one-dimensional conditionals, typically ordered along vector coordinates or pixels.
Our RNADE-style head follows this recipe but conditions the masked MLP on the pooled context via FiLM layers~\cite{perez2018film}: the context embedding is mapped to per-layer scales and shifts that modulate hidden activations, so every conditional $p(z_d \mid z_{<d}, \text{context})$ is explicitly context-aware.
This gives a normalized density over teacher embeddings while still allowing us to draw diverse samples by sampling dimensions sequentially, making the autoregressive head an example of a model where the distribution is primarily explored through explicit sampling rather than a closed-form multivariate mixture.

\textbf{Mixture Density Networks.}
MDNs~\cite{bishop1994mdn} attach mixture weight, mean, and variance heads to a neural network, yielding a cheap way to capture multimodality.
It is useful to distinguish \emph{unimodal} distributions (a single peak, such as a single Gaussian) from \emph{multimodal} ones (several separated peaks).
Because MDNs parameterize a mixture of Gaussians, they can in principle represent multimodal output distributions; in practice, if most mixture weights collapse onto one component the effective distribution becomes unimodal.
In our setting the predictor keeps the I-JEPA transformer stack but emits $(\pi_k,\mu_k,\sigma_k)$ per token in a single forward pass; this already defines an explicit parametric distribution, and we can obtain as many samples as desired without re-running the encoder, simply by resampling from the mixture.

\textbf{Normalizing flows and flow matching.}
RealNVP~\cite{dinh2017realnvp} introduced affine coupling layers that are exactly invertible, so a simple base density (e.g., a standard Gaussian) is transformed into a complex one while keeping track of the exact change of variables.
ActNorm layers~\cite{kingma2018glow} stabilize training by learning per-channel scales and offsets.
Flow matching~\cite{lipman2023flowmatching} trains such flows by supervising a vector field that transports noise toward data along simple stochastic interpolants between the two.
In our JEPA head, a stack of coupling layers conditions on context features and time embeddings; a single forward pass provides both an explicit likelihood for a teacher token and a mapping from Gaussian noise to a latent sample, so the full distribution is modeled analytically after one run rather than only through repeated sampling.

\textbf{Diffusion models.}
Denoising diffusion models~\cite{ho2020ddpm,song2020ddim} define a forward Markov chain that gradually corrupts data with Gaussian noise and train a neural network to reverse this process by predicting the noise at each step.
We follow the DDPM noise-prediction formulation~\cite{ho2020ddpm} with DDIM-style deterministic sampling~\cite{song2020ddim}: at generation time the model is applied multiple times along a fixed noise schedule to iteratively denoise a latent from pure noise back to a plausible teacher embedding.
Placed after the JEPA encoder, diffusion heads treat each teacher token as ground-truth clean data and learn to reverse progressively noised latents; the resulting distribution is accessed implicitly by running the denoising process multiple times with different noise seeds, making diffusion the most sampling-intensive of our predictors but also the one that empirically covers modes most thoroughly.

\section{Methodology}
\subsection{Principle of JEPA}
I-JEPA operates in latent space: a momentum-updated teacher encodes full images into patch embeddings, while a student sees only masked context patches.
The predictor receives context features plus positional target queries and outputs a prediction for each masked target embedding; losses compare predicted and teacher target embeddings without pixel reconstruction.
This removes low-level pixel pressure and focuses on semantic consistency; masking budget (area/AR/num blocks) controls how ambiguous the targets are.
Because multiple completions are plausible, deterministic predictors can collapse to a single modeâ€”hence our probabilistic heads.
In every variant, training alternates: (1) sample context/target masks, (2) teacher encodes the full image, (3) student encodes context only, (4) predictor outputs a distribution for each target token, (5) loss is applied in latent space, (6) EMA updates the teacher.

\subsection{Backbone and Masking}
We closely follow the method from the original I-JEPA paper. All variants share the same ViT encoder~\cite{dosovitskiy2020vit}: image size $64{\times}64$, patch size $8$, hidden dimension $384$, $10$ transformer layers, and $8$ heads (MDN uses identical sizes; the autoregressive variant uses $600$ hidden dim and $6$ layers to match its RNADE head).
For each image, a large context block is sampled, and four target blocks are selected with area $\in[0.15,0.20]$ and aspect ratio $\in[0.75,1.50]$; at least $8$ context patches are enforced.
The student encodes only visible context, while the EMA teacher encodes full images.
Target positional encodings are added to learned query tokens for each masked patch.
Figure~\ref{fig:masking} illustrates a sampled context (green) and target (red) layout on Tiny ImageNet.

\subsection{Dataset and Preprocessing}
We train on Tiny ImageNet-100 (100 classes, $\sim$50k train / 5k val images at $64{\times}64$)~\cite{le2015tinyimagenet} using the Hugging Face \texttt{zh-plus/tiny-imagenet} split with classes filtered to $<100$.
Per-channel mean and std are estimated on a 2{,}000-image subsample.
Training augmentations: random resized crop to $64{\times}64$ and horizontal flip; validation uses a resize only.
The random-resized-crop augmentation first samples a sub-rectangle covering $80$--$100\%$ of the original area (with slight aspect-ratio jitter), then rescales that patch back to $64{\times}64$, effectively implementing random zoom-and-translate.

\subsection{Training Hyperparameters}
All runs use AdamW~\cite{loshchilov2018adamw} with gradient clipping ($\|g\|\_2\le3$).
EMA momentum follows a cosine schedule from $0.998$ to $1.0$.
Flow and diffusion heads train with batch size $600$ and base LR $5\!\times\!10^{-5}$; MDN uses batch $360$ and LR $3\!\times\!10^{-4}$; the autoregressive head uses batch $128$ and LR $1\!\times\!10^{-4}$.
Diffusion inference uses $50$ DDIM-like steps~\cite{song2020ddim}; flows clamp coupling scales to $\pm 2$ for stability.

\section{Predictor Case Studies}
We separate the qualitative and quantitative discussion by predictor to make the report approachable to re-implement each variant.  Every subsection summarizes the motivation, implementation strategy, diagnostics, and the concrete evaluation numbers.

\subsection{Deterministic I-JEPA Baseline}
The baseline follows~\cite{assran2023jepa}: the student predictor ingests context tokens and positional queries, applies two transformer blocks, and emits a single embedding per target. Additionally to the paper, we've implemented alignment, variance, and covariance terms to track wether the model naturally produces uncorrelated features. And that appeared true, making it easier to us to model distributions under the i.i.d. assumption for latent random variables. EMA momentum ramps from $0.998$ to $1.0$ over the 200-epoch schedule. Because the output distribution collapses to a single vector, ambiguous regions such as textured backgrounds or heavily occluded objects are forced into a thin ridge, as visible in the bottom row of Figure~\ref{fig:dist_comparison}.  We did not have access to the raw TensorBoard logs, so \texttt{analysis/eval\_baseline\_loss.py} replays the validation loop on Tiny ImageNet-100 and produces the scalar panel in Figure~\ref{fig:losses}.  The only available checkpoint uses a 256-dimensional ViT (inherited from another course project), and when evaluated with our unified kNN pipeline it tops out at $9.84\%$ (Table~\ref{tab:results}). This modest score is nevertheless important: it quantifies the deterministic ceiling and highlights why probabilistic predictors are desirable.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/i-jepa_scheme.jpeg}
    \caption{High-level schematic of the deterministic I-JEPA architecture. The student sees only context patches, the teacher encodes the full image, and the predictor regresses target embeddings.}
    \label{fig:baseline_scheme}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/baseline_distribution.png}
    \caption{Latent distribution for the deterministic I-JEPA baseline. Points represent predictor samples in the first two principal components; teacher tokens are overlaid in cyan.}
    \label{fig:baseline_distribution}
\end{figure}

\subsection{Mixture Density Network (MDN)}
The MDN head keeps the same backbone but widens the predictor: two transformer layers process the concatenated context and target queries before three linear heads emit $(\pi_k,\mu_k,\sigma_k)$ for each token.  We fix $K=5$ diagonal Gaussians and regularize the log-variance to avoid degenerate distributions.  The training script (\texttt{MDN/mdn.py}) logs mixture entropy, $\sigma$ histograms, and alignment loss; gradients remain well-behaved thanks to the diagonal covariance assumption.  Figure~\ref{fig:mdn_pca} shows that the component means, random samples, and teacher targets align in the principal subspace, while Figure~\ref{fig:mdn_hex} visualizes the multi-modal support once 4,096 draws are projected to 2D.  The new comparison grid (Figure~\ref{fig:dist_comparison}, top row) confirms that the MDN head spreads probability mass over multiple semantic clusters instead of collapsing to a single ridge.  Using the shared evaluation script, the MDN predictor achieves $13.96\%$ kNN@1, a $+4.1$ point gain over the deterministic reference despite identical data and augmentation.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/mdn_scheme.jpeg}
    \caption{Schematic of the MDN predictor head. The JEPA backbone provides context and target queries; the MDN head outputs mixture weights and Gaussian parameters for each masked token.}
    \label{fig:mdn_scheme}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/mdn_distribution.png}
    \caption{MDN latent distribution: PCA-hexbinned samples from the learned mixture, with teacher tokens in cyan. Multiple dense lobes reflect genuinely multi-modal predictions.}
    \label{fig:mdn_distribution}
\end{figure}

\subsection{Autoregressive RNADE Head}
Our RNADE-style predictor operates on normalized teacher embeddings and factorizes the joint density into one-dimensional conditionals.  A masked MLP with FiLM modulation incorporates the pooled context, while ActNorm ensures numerically stable conditioning.  Compared to the MDN, this design increases expressivity at the cost of sequential sampling: for each embedding dimension we sample a component via the learned mixture weights and draw from the corresponding Gaussian before proceeding to the next dimension.  The notebook \texttt{autoregression/autoregression.ipynb} documents the full training recipe, but we also extracted the reusable core (backbone, predictor, masking utilities) into \texttt{autoregression/rnade\_model.py} so scripted evaluations can import it directly.  Training for 20 epochs with AdamW ($1\!\times\!10^{-4}$, weight decay $0.05$) delivered smooth convergence and an effective component count of $\approx1.8$.  Our original TensorBoard snapshot reported $8.40\%$ kNN@1; re-running the evaluation with \texttt{analysis/compute\_knn.py} raises the official number to $11.92\%$, reflecting the exact Tiny ImageNet split used by the other predictors.  The middle row of Figure~\ref{fig:dist_comparison} illustrates the resulting distribution: it remains multi-modal but is more elongated than the MDN because each dimension is modeled separately.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/autoregression_scheme.jpeg}
    \caption{Schematic of the autoregressive (RNADE-style) head. Context features condition a masked MLP that outputs mixture parameters coordinate-wise, implementing an autoregressive density over the embedding.}
    \label{fig:autoregressive_scheme}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/autoregressive_distribution.png}
    \caption{Autoregressive (RNADE) latent distribution in PCA space. Compared to MDN, the density is more elongated, reflecting the coordinate-wise factorization.}
    \label{fig:autoregressive_distribution}
\end{figure}

\subsection{RealNVP Flow Matching Head}
The flow-matching head augments the deterministic predictor with six affine coupling layers whose conditioners observe both context features and time embeddings.  We follow~\cite{lipman2023flowmatching} by training with stochastic interpolants between the teacher tokens and Gaussian noise, supervised by a denoising score-matching objective.  Per-user feedback, we filtered the TensorBoard runs inside \texttt{flow\_matching/runs/} and ignored truncated logs that only start after $6.5$M steps; \texttt{analysis/plot\_losses.py} performs this filtering automatically before plotting the curves in Figure~\ref{fig:losses}.  The best flow run reached $18.48\%$ kNN@1 at step $7.94$M, with validation NLL around $-71.5$ and relatively flat token cosine similarity.  Sampling is single-shot: we draw Gaussian noise, push it through the inverse coupling network, and obtain predictor embeddings that can be compared to the teacher tokens or visualized via PCA.  The flow head is therefore a good compromise between the MDN (cheap but approximate) and the diffusion head (expensive but most accurate).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/flow_matching_scheme.jpeg}
    \caption{Schematic of the flow-matching head. Coupling layers transform Gaussian noise into latent predictions, conditioned on context features and time embeddings.}
    \label{fig:flow_scheme}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/flow_distribution_placeholder.png}
    \caption{Placeholder: flow-matching latent distribution. This slot is reserved for a PCA-based histogram of flow samples once the final flow checkpoints are exported into this repository.}
    \label{fig:flow_distribution}
\end{figure}

\subsection{Diffusion over Embeddings}
Our diffusion variant treats each teacher token as a ``clean'' latent and trains a U-Net-like head to denoise embeddings corrupted by a linear beta schedule.  Architecture-wise, the predictor mirrors standard vision diffusion models: sinusoidal timestep embeddings modulate residual attention blocks, and context summaries FiLM the intermediate features.  Training mirrors the flow configuration (batch 600, AdamW $5\!\times\!10^{-5}$) but optimizes the mean-squared error between predicted noise and true perturbation rather than a flow-matching score.  The denoising procedure at inference time follows DDIM~\cite{song2020ddim}; we default to 50 steps but also provide a 25-step fast mode that preserves $>95\%$ of the final accuracy.  Diffusion logs include token MSE, cosine similarity, and kNN@1 every 200 steps (Figure~\ref{fig:knn}), clearly outperforming the other heads with a peak of $22.36\%$.  Qualitatively, diffusion samples cover the teacher distribution even better than the flow head, and the gradual denoising process gives us an interpretable trajectory between noise and prediction.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/diffusion_scheme.jpeg}
    \caption{Schematic of the diffusion head. Teacher embeddings are treated as clean data; noise schedules and a U-Net-like predictor implement iterative denoising in latent space.}
    \label{fig:diffusion_scheme}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/diffusion_distribution_placeholder.png}
    \caption{Placeholder: diffusion latent distribution. To be replaced by a PCA-hexbin visualization of diffusion samples once the corresponding checkpoints are added to the repository.}
    \label{fig:diffusion_distribution}
\end{figure}


\section{GitHub Repository}
Repository: \url{<insert GitHub URL>} (replace with the public link before submission).
Folders map 1:1 to predictor variants (\texttt{I-JEPA}, \texttt{MDN}, \texttt{autoregression}, \texttt{flow\_matching}, \texttt{diffusion}); each contains its training script, available checkpoints, and configuration files.
Loss logs for flow/diffusion reside in \texttt{flow\_matching/runs/} and \texttt{diffusion/scalars (2).json}, and the deterministic baseline checkpoint is versioned in \texttt{BionicEye/representation/current/checkpoints}.
The \texttt{analysis/} folder centralizes evaluation utilities: \texttt{plot\_losses.py} (Figure~\ref{fig:losses}), \texttt{plot\_knn.py} (Figure~\ref{fig:knn}), \texttt{compute\_knn.py} (Table~\ref{tab:results}), \texttt{eval\_baseline\_loss.py} (baseline sweep), and \texttt{compare\_distributions.py} (Figure~\ref{fig:dist_comparison}).
MDN-specific diagnostics live in \texttt{MDN/visualize\_mdn.py} and \texttt{analysis/mdn\_distribution\_3d.py}, and the masking illustration is regenerated via \texttt{analysis/mask\_overlay.py}.

\section{Experiments and Evaluation}
\subsection{Setup}
Hardware: CUDA when available, otherwise MPS/CPU fallbacks (all scripts auto-detect).
Optimization: AdamW with gradient clipping ($\|g\|\_2\le3$); EMA teacher updates follow cosine momentum schedule.
Evaluation: freeze the student backbone, extract patch embeddings, pool to a single vector per image, and compute kNN@1 on the validation set with $k=20$ neighbors.
Validation is run every 200 training iterations for flow/diffusion (as logged in TensorBoard) and once per epoch for the autoregressive head.
Additional diagnostics: mixture entropy and $\sigma$ histograms (MDN), log-determinants (flow), and token MSE / cosine similarity (diffusion).
Our shared evaluation pipeline (\texttt{analysis/compute\_knn.py}) re-extracts train/val embeddings for every checkpoint, normalizes them, and performs chunked cosine kNN so the reported scores in Table~\ref{tab:results} are directly comparable; \texttt{analysis/eval\_baseline\_loss.py} applies the deterministic losses over the validation split to produce the baseline panel in Figure~\ref{fig:losses}.

\subsection{Results}
\begin{table}[t]
\centering
\begin{tabular}{>{\raggedright}p{1.8cm}ccp{2.0cm}}
\toprule
Model & Batch & Epochs/Steps & Best val kNN@1 (\%) \\
\midrule
Baseline I-JEPA$^\dagger$ & 600 & $1.3\!\times\!10^5$ samples & 9.84 \\
MDN ($K{=}5$) & 360 & 50 & 13.96 \\
Autoregressive & 128 & 20 & 11.92 \\
Normalizing flow & 600 & $7.9$M steps & 18.48 \\
Diffusion & 600 & $2.0$M steps & 22.36 \\
\bottomrule
\end{tabular}
\caption{Validation kNN@1 over Tiny ImageNet-100 embeddings using a unified feature-extraction pipeline (analysis/compute\_knn.py). $^\dagger$Baseline checkpoint is the available 256-dim ViT run from \texttt{BionicEye/representation}, so its capacity slightly differs from the 384-dim probabilistic heads.\label{tab:results}}
\end{table}

Diffusion still leads, suggesting that iterative denoising over latents captures multimodal structure better than single-step flows or factorized mixtures.
Normalizing flows outperform the autoregressive head, likely due to exact likelihood training and richer coupling transforms.
The autoregressive model now evaluates at $11.92\%$ once we re-ran kNN on the same Tiny ImageNet shards as the other predictors, and the MDN head reaches $13.96\%$ despite not logging validation curves during training.
Figure~\ref{fig:losses} contrasts diffusion train/val losses with flow train/val NLL plus the deterministic baseline sweep extracted by \texttt{analysis/eval\_baseline\_loss.py}, highlighting how we filtered TensorBoard event files to remove the truncated $6.5$M-step flow run.
Figure~\ref{fig:knn} overlays validation kNN@1 for diffusion and flow while annotating the scalar checkpoints for MDN, RNADE, and the deterministic baseline derived from \texttt{analysis/compute\_knn.py}.
Qualitative diagnostics for the probabilistic heads are given in Figures~\ref{fig:mdn_pca}--\ref{fig:mdn_hex} and the new distribution grid (Figure~\ref{fig:dist_comparison}), which re-samples the checkpoints directly.

\section{Visualizations}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/combined_losses.png}
    \caption{Training/validation curves extracted from logged scalars: the deterministic baseline sweep (left, per-batch validation loss), diffusion (center: train/val loss) and flow matching (right: train/val NLL) over training steps (millions).}
    \label{fig:losses}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/pca_means_samples_targets.png}
    \caption{MDN qualitative analysis: PCA of component means $\mu_k$, samples, and teacher targets for one batch (Tiny ImageNet-100 validation).}
    \label{fig:mdn_pca}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/combined_knn.png}
    \caption{Validation kNN@1 across flow matching and diffusion runs, with scalar checkpoints for MDN, RNADE, and the deterministic baseline. Diffusion climbs faster and higher (22.36\% peak).}
    \label{fig:knn}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/mdn_samples_hex.png}
    \caption{MDN multimodal samples (4{,}096 draws) projected to 2D via PCA and rendered as a hexbin heatmap, showing multi-cluster support.}
    \label{fig:mdn_hex}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/mdn_distribution_3d.png}
    \caption{Isometric 3D view of MDN samples projected onto the first three principal components; each cluster corresponds to a distinct mixture mode.}
    \label{fig:mdn_iso}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/mask_overlay.png}
    \caption{Masking policy illustration: context patches (green) and targets (red) on Tiny ImageNet ($8\\times 8$ grid, $64\\times 64$ image).}
    \label{fig:masking}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.92\textwidth]{figures/distribution_comparison.png}
    \caption{Projected embedding distributions sampled directly from checkpoints. Left column: 2D PCA hexbins with teacher targets (cyan); right column: 3D PCA scatter plots. Rows correspond to the MDN head (top), autoregressive RNADE head (middle), and the deterministic I-JEPA baseline (bottom). Probabilistic heads form multi-modal, high-entropy clusters, whereas the deterministic predictor collapses to a single ridge.}
    \label{fig:dist_comparison}
\end{figure*}

Additional MDN diagnostics (entropy, mixture weights, $\sigma$ histograms) reside in \texttt{figures/entropy.png}, \texttt{figures/mixture\_weights.png}, and \texttt{figures/sigma\_distribution.png}; Figure~\ref{fig:mdn_iso} shows the same samples in an isometric 3D view for better intuition about the mixture modes.
These plots reveal broad uncertainty mass (high entropy and wide $\sigma$ range), consistent with the goal of capturing multimodal completions.

\section{Analysis and Observations}
\begin{itemize}
    \item \textbf{Uncertainty modeling.} MDN and RNADE heads now have both quantitative (Table~\ref{tab:results}) and qualitative (Figure~\ref{fig:dist_comparison}) evidence showing wide, multi-modal support that aligns with teacher embeddings. Their entropy advantage over the deterministic baseline is visible in the hexbins.
    \item \textbf{Training stability.} Flow logs still report large gradient norms; clipping at $3$ helps, but coupling-scale clamping ($\pm2$) may underfit. Figure~\ref{fig:losses} confirms that our TensorBoard filters remove the corrupted 6.5M-step segment so only consistent runs remain.
    \item \textbf{Diffusion quality.} Despite higher token MSE, diffusion achieves the highest kNN@1, suggesting that iterative latent denoising preserves semantics better than single-pass predictors.
    \item \textbf{Masking sensitivity.} All variants reuse the same block-sampling policy; future ablations include varying target area, number of blocks, conditioning flows/diffusion on shared context encoders, or biasing the random rectangles toward object centers.
    \item \textbf{Compute budget.} MDN (batch 360) and autoregression (batch 128) are modest, whereas flows/diffusion need batch 600 and multi-million steps. The shared kNN extraction now adds $\sim$12 minutes per model using \texttt{analysis/compute\_knn.py} on MPS.
    \item \textbf{Optimization details.} AdamW weight decay follow-through matters: removing decay dropped flow kNN by $\approx$1.5 points; lowering gradient clipping destabilized diffusion. Further hyper-parameter sweeps should include EMA schedules for probabilistic heads.
    \item \textbf{Sampling cost.} Diffusion inference uses 50 DDIM steps; reducing to 25 halves runtime with minor kNN changes (not logged). MDN and RNADE sample in a single forward pass; flows inversely map Gaussian noise to latents with six coupling layers.
    \item \textbf{Log coverage.} Flow/diffusion have complete TensorBoard/json logs, enabling Figures~\ref{fig:losses}--\ref{fig:knn}; for MDN/autoregressive we rely on checkpoints, \texttt{analysis/eval\_baseline\_loss.py}, and \texttt{analysis/compare\_distributions.py} to regenerate diagnostics.
    \item \textbf{Reproduction.} Regenerate Figures~\ref{fig:losses}--\ref{fig:dist_comparison} via \texttt{analysis/plot\_losses.py}, \texttt{analysis/plot\_knn.py}, \texttt{analysis/compute\_knn.py}, \texttt{analysis/compare\_distributions.py}, \texttt{MDN/visualize\_mdn.py}, and \texttt{analysis/mask\_overlay.py}. All scripts read checkpoints/logs directly and write into \texttt{report/figures/}.
\end{itemize}

\section{Next Steps}
\begin{itemize}
    \item Retrain or fine-tune the deterministic I-JEPA baseline with the same 384-dim architecture to make comparisons perfectly apples-to-apples.
    \item Instrument MDN and autoregressive training loops with TensorBoard logging so future reruns do not rely solely on checkpoint-level statistics.
    \item Add downstream linear probes / segmentation transfers and study whether probabilistic heads improve dense recognition tasks.
    \item Replace placeholder repository URLs with the final GitHub link and ensure all citations include DOI/URL metadata.
\end{itemize}

\section{Conclusion}
We extended I-JEPA with four probabilistic predictors and evaluated them on Tiny ImageNet-100 using identical masking, EMA schedules, and a shared kNN@1 pipeline.
Diffusion over embeddings currently performs best (22.36\%), followed by normalizing flows (18.48\%), the MDN head (13.96\%), and the autoregressive RNADE head (11.92\%); the available deterministic baseline checkpoint (256-dim ViT) reaches 9.84\%.
New figures report true TensorBoard scalars, filtered flow runs, and projected latent distributions sampled directly from checkpoints, enabling reproducible comparisons.
Future work includes retraining the deterministic baseline at 384 dimensions, logging MDN/autoregressive curves during training, expanding downstream evaluations, and finalizing the public repository link with full instructions.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
